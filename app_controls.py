chat_model = "llama3-8b-8192"

# app params
memory_len = 4

# llm params
gen_max_tokens = 200
temperature = 0.1
top_p=0.1
frequency_penalty=2
presence_penalty=2